{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cell-image-completion-v4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulxiong/ai-challenge-mars/blob/master/Cell_image_completion_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5tQNQMUClMn",
        "colab_type": "text"
      },
      "source": [
        "# Download Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE07L_ZhCqgp",
        "colab_type": "code",
        "outputId": "e47f43d6-f7c5-4850-d9e0-5b0191ee50e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "GPATH=\"/content/gdrive/My Drive/aiml/OmdenaMars/\"\n",
        "\n",
        "import os\n",
        "GPATH=\"/content/gdrive/My Drive/aiml/OmdenaMars/\"\n",
        "os.environ[\"GPATH\"] = GPATH\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RLgYRNJWid0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f518f2b9-2b0e-4145-da0d-1b1a75b95ac0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJHAcbW3O6Zv",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OJkPPKsDHfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_PATH=\"/content/data/\"\n",
        "os.environ[\"BASE_PATH\"] = BASE_PATH\n",
        "\n",
        "TRAIN_PATH=BASE_PATH+\"train/\"\n",
        "os.environ[\"TRAIN_PATH\"] = TRAIN_PATH\n",
        "\n",
        "TEST_PATH=BASE_PATH+\"test/\"\n",
        "os.environ[\"TEST_PATH\"] = TEST_PATH\n",
        "\n",
        "TMP_PATH=BASE_PATH+\"tmp/\"\n",
        "os.environ[\"TMP_PATH\"] = TEST_PATH\n",
        "\n",
        "WEIGHTS_PATH=GPATH+\"weights/image-completion-v4.1/\"\n",
        "os.environ[\"WEIGHTS_PATH\"] = WEIGHTS_PATH\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPS3qT1lDccs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pathlib\n",
        "from shutil import copyfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "pathlib.Path(TRAIN_PATH).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(TMP_PATH).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(WEIGHTS_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not os.path.isfile(BASE_PATH+\"natural.tgz\"):\n",
        "    print(\"Copying natural.tgz to local folder\")\n",
        "    copyfile(GPATH+\"natural.tgz\", BASE_PATH+\"natural.tgz\")\n",
        "if not os.path.isfile(BASE_PATH+\"techno.tgz\"):\n",
        "    print(\"Copying techno.tgz to local folder\")\n",
        "    copyfile(GPATH+\"techno.tgz\", BASE_PATH+\"techno.tgz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKmQoU4PyVXX",
        "colab_type": "text"
      },
      "source": [
        "Extract \"techno signature\" files to test path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSuiEHy9_KOw",
        "colab_type": "code",
        "outputId": "ce70e343-6ead-41da-b73f-b2515f932196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        }
      },
      "source": [
        "!rm -rf /content/data/test\n",
        "!echo \"Extracting techno.tgz\"\n",
        "!tar -xzf \"$BASE_PATH\"\"techno.tgz\" -C $TMP_PATH\n",
        "!echo \"Copying Techno folder as test\"\n",
        "#boostx : remake the test path with techno\n",
        "#!cp -r \"/content/data/tmp/content/gdrive/My Drive/aiml/OmdenaMars/data/techno\" \"$TEST_PATH\"\n",
        "!cp -r \"/content/data/tmp/content/gdrive/My Drive/aiml/OmdenaMars/data\" \"$TEST_PATH\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting techno.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8DPyI5ZCWsk",
        "colab_type": "text"
      },
      "source": [
        "Extract natural files with out black borders to train path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCeMJfvzAx5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "\n",
        "!rm -rf /content/data/train\n",
        "!mkdir /content/data/train\n",
        "!echo \"Extracting natural.tgz\"\n",
        "!tar -xzf \"$BASE_PATH\"\"natural.tgz\" -C $TMP_PATH\n",
        "#boostx : don't know why the system add some .xxx.png files, so delete it.\n",
        "!rm -rf /content/data/train/._*\n",
        "!rm -rf /content/data/train/.DS_Store\n",
        "#boostx : end\n",
        "\n",
        "print(\"Moving Natural File\")\n",
        "count = 204800\n",
        "for file in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/natural/\"):\n",
        "  #boostx : don't check the coodinations \n",
        "  #  f1 = file.split(\"_\")\n",
        "  #  xVal = int(f1[2])\n",
        "  #  yVal = int(f1[4].split(\".\")[0])\n",
        "  #  if (xVal >= 8000) and (yVal>=5000):\n",
        "  #      if(xVal <= 15000) and (yVal <= 30000):\n",
        "  #boostx : end\n",
        "            shutil.move(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/natural/\"+file, TRAIN_PATH+file)\n",
        "            count -= 1\n",
        "            if (count <= 0):\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZw61vQaC4_w",
        "colab_type": "text"
      },
      "source": [
        "In a production data set anomalies are already mixed with natural images. So lets copy anomaly images also to natural images and train on this data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hCd2Sx5Djhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "#boostx : add natural data to TESTPATH\n",
        "!rm -rf /content/data/test/natural\n",
        "!mkdir /content/data/test/natural\n",
        "! cp /content/data/train/*.*  /content/data/test/natural/\n",
        "\n",
        "for dir in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"):\n",
        "    #boostx : don't know why there is ._9, just ignore it.\n",
        "    if dir == '._9' or dir == '.DS_Store':\n",
        "      print(str(dir))\n",
        "      break;\n",
        "    #boostx : end\n",
        "    print(\"Moving Files from:\", dir+\":\")\n",
        "    for file in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"+dir):\n",
        "        #print(file)\n",
        "        shutil.copy(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"+dir+\"/\"+file, TRAIN_PATH+file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdas3iB6D8KD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "train_datagen = ImageDataGenerator()\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory   = BASE_PATH,       # this is the target directory\n",
        "    target_size = (128, 128, 1)[:-1],    # all images will be resized to 64x64\n",
        "    batch_size  = 128,\n",
        "    color_mode  = \"grayscale\",           # We use a grayscale dataset\n",
        "    classes=[\"train\"],\n",
        "    class_mode  = None                   # We do not need to get any label => Everything is healthy\n",
        ")  \n",
        "\n",
        "#train_generator.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na7D5srsVYkl",
        "colab_type": "text"
      },
      "source": [
        "# AnomalyDetector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX_DnqCY2c6K",
        "colab_type": "text"
      },
      "source": [
        "## Common Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZgocvPfTGxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Conv1D, UpSampling2D\n",
        "from keras.backend import clip\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.constraints import Constraint\n",
        "from keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "from keras.utils. generic_utils import Progbar\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gN4QajKJBl4",
        "colab_type": "text"
      },
      "source": [
        "## Common Defines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuVEbKLDJACS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "IMAGE_SIZE = 128\n",
        "#boostx : use 128 instead of 32\n",
        "MASK_SIZE = 32\n",
        "#MASK_SIZE = 128\n",
        "#boostx end\n",
        "LAMBDA = 0.9\n",
        "\n",
        "SAVE_EVERY_EPOCH = 5\n",
        "TEST_EVERY_EPOCH = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2CpWRqL2f7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from: https://stackoverflow.com/questions/42264567/keras-ml-library-how-to-do-weight-clipping-after-gradient-updates-tensorflow-b\n",
        "class WeightClip(Constraint):\n",
        "    '''Clips the weights incident to each hidden unit to be inside a range\n",
        "    '''\n",
        "    def __init__(self, c=1):\n",
        "        self.c = c\n",
        "\n",
        "    def __call__(self, p):\n",
        "        return K.clip(p, -self.c, self.c)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'name': self.__class__.__name__,\n",
        "                'c': self.c}\n",
        "\n",
        "\n",
        "def defineModel():\n",
        "    \n",
        "    kernelInitializer = RandomNormal(mean=0.0, stddev=1, seed=None)\n",
        "\n",
        "    #where Conv(k; d; s; c) denotes a convolutional layer with kernel size k x k, dilation rate d, stride s and c output channels.\n",
        "    model = Sequential()\n",
        "\n",
        "    #Conv(5; 1; 1; 32)\n",
        "    model.add(Conv2D(input_shape = (128,128,1), \n",
        "                     filters=32, kernel_size=5, dilation_rate=1, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 1; 1; 64)\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, dilation_rate=1, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 1; 1; 64)\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, dilation_rate=1, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 1; 2; 128)\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, dilation_rate=1, strides=2, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 1; 1; 128)\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, dilation_rate=1, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 1; 1; 128)\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, dilation_rate=1, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 2; 1; 128)\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, dilation_rate=2, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 4; 1; 128)\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, dilation_rate=4, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 8; 1; 128)\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, dilation_rate=8, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 16; 1; 128)\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, dilation_rate=16, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "    #model.add(Conv2D(filters=128, kernel_size=3, dilation_rate=8, strides=1))\n",
        "\n",
        "    #Conv(3; 1; 1; 128)\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, dilation_rate=1, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 1; 1; 128)\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, dilation_rate=1, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Bilinear Upscaling(2x)\n",
        "    model.add(UpSampling2D(size=2, interpolation=\"bilinear\"))\n",
        "\n",
        "    #Conv(3; 1; 1; 64)\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, dilation_rate=1, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 1; 1; 64)\n",
        "    model.add(Conv2D(filters=32, kernel_size=3, dilation_rate=1, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 1; 1; 32)\n",
        "    model.add(Conv2D(filters=32, kernel_size=3, dilation_rate=1, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 1; 1; 16)\n",
        "    model.add(Conv2D(filters=16, kernel_size=3, dilation_rate=1, strides=1, padding=\"same\", activation=\"elu\"))\n",
        "\n",
        "    #Conv(3; 1; 1; 1) + Clip(-1; 1)\n",
        "    model.add(Conv2D(filters=1, kernel_size=3, dilation_rate=1, strides=1, padding=\"same\", W_constraint = WeightClip(1), activation=\"elu\"))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVhSnG6DViXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model, vis_utils\n",
        "from IPython.display import Image\n",
        "from IPython.display import display\n",
        "\n",
        "import IPython\n",
        "\n",
        "model = defineModel()\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "display(Image(retina=True,filename='model.png'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deQLEfenfUFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf \n",
        "\n",
        "\n",
        "def getMaskArray(aSize, mSize):\n",
        "    m = np.ones((aSize, aSize))\n",
        "    haSize = int(aSize/2)\n",
        "    hmSize = int(mSize/2)\n",
        "    m[haSize-hmSize:haSize+hmSize, haSize-hmSize:haSize+hmSize] = 0\n",
        "    return m\n",
        "\n",
        "\n",
        "def getLoss(aSize, mSize):\n",
        "    \n",
        "    m1 = getMaskArray(aSize,mSize)\n",
        "    m2 = np.repeat(m1[np.newaxis, :, :, np.newaxis], BATCH_SIZE, axis=0)\n",
        "    mask = K.variable(m2)\n",
        "    invMask = K.variable(1-mask)\n",
        "    imgSize = K.variable(IMAGE_SIZE)\n",
        "    batchSize = K.variable(BATCH_SIZE)\n",
        "    lam = K.variable(LAMBDA)\n",
        "    def anoLoss(y_true, y_pred):\n",
        "        d1 = tf.math.multiply(mask, (y_true - tf.math.multiply(invMask,y_pred)))\n",
        "        d2 = tf.math.multiply(invMask, (y_true - tf.math.multiply(invMask,y_pred)))\n",
        "        #v1 = (lam*tf.norm(d1,ord=1))/(imgSize*imgSize)\n",
        "        #v2 = (lam*tf.norm(d2,ord=1))/(imgSize*imgSize)\n",
        "        v1 = (lam*tf.norm(d1,ord=1))\n",
        "        v2 = (lam*tf.norm(d2,ord=1))\n",
        "        r = (v1+v2)/batchSize\n",
        "        #d = tf.Print(r, [r], \"Inside loss function\")\n",
        "        return r\n",
        "        \n",
        "    return anoLoss\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_tF6cyt_DUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = defineModel()\n",
        "\n",
        "#ADAM optimizer \n",
        "adam = Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=pow(10,-8))\n",
        "\n",
        "model.compile(optimizer=adam,\n",
        "              loss=getLoss(IMAGE_SIZE,MASK_SIZE)) # Call the loss function with the selected layer\n",
        "\n",
        "#model.compile(optimizer=\"adam\",\n",
        "#              loss=\"mse\",\n",
        "#              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfzZ0va4FF0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#boostx : don't know why the system add some .xxx.png files, so delete it.\n",
        "!rm -rf /content/data/train/._*\n",
        "#boostx : end\n",
        "\n",
        "\n",
        "\n",
        "X_train_generator = train_generator\n",
        "batch_size = X_train_generator.batch_size        \n",
        "n_steps = int(X_train_generator.samples / batch_size)\n",
        "\n",
        "mask = getMaskArray(IMAGE_SIZE, MASK_SIZE)\n",
        "#invMask = 1-mask\n",
        "mulMask = np.repeat(mask[np.newaxis, :, :, np.newaxis], BATCH_SIZE, axis=0)\n",
        "\n",
        "\n",
        "SAVE_EVERY_EPOCH=5\n",
        "reloadWeights = True\n",
        "maxEpoch = 100\n",
        "field_names = ['epoch', 'file-name','loss']\n",
        "_lastEpoch = 0\n",
        "\n",
        "#Initialize train status file\n",
        "if (not os.path.isfile(WEIGHTS_PATH+\"train-status.csv\")) or (reloadWeights == False):\n",
        "    with open(WEIGHTS_PATH+\"train-status.csv\",\"w\") as trainStatusFile:\n",
        "        tStatusLogger = csv.DictWriter(trainStatusFile, fieldnames=field_names)\n",
        "        tStatusLogger.writeheader()\n",
        "else:\n",
        "    df = pd.read_csv(WEIGHTS_PATH+\"train-status.csv\")\n",
        "    if df.shape[0] > 0:\n",
        "        _lastEpoch = df[\"epoch\"].iloc[-1]\n",
        "        lastFileName = df[\"file-name\"].iloc[-1]\n",
        "        print(\"Last Completed Epoch:\", _lastEpoch)\n",
        "        print(\"Loading Weights from:\", lastFileName)\n",
        "        model.load_weights(WEIGHTS_PATH+lastFileName)\n",
        "\n",
        "_lastEpoch += 1\n",
        "for epoch in range(_lastEpoch, maxEpoch+1):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    n_iter = int(X_train_generator.samples / BATCH_SIZE)\n",
        "    progress_bar = Progbar(target=n_iter)\n",
        "    lossHist = []\n",
        "    for index in range(n_iter):\n",
        "        #boostx : don't know why the system add some .xxx.png files, so delete it.\n",
        "        #print(\"boostx :\"+str(index))\n",
        "        try:\n",
        "          image_batch = (X_train_generator.next().astype(np.float32) - 127.5) / 127.5\n",
        "          if (image_batch.shape[0] != BATCH_SIZE):\n",
        "              #Incomplete last batch, skip to next batch\n",
        "              image_batch = (X_train_generator.next().astype(np.float32) - 127.5) / 127.5\n",
        "          \n",
        "          #image_batch = image_batch[:,:,:,0]\n",
        "          masked_batch = np.multiply(image_batch,mulMask)\n",
        "          X = masked_batch\n",
        "          y = image_batch\n",
        "          lossVal = model.train_on_batch(X,y)\n",
        "          progress_bar.update(index+1, values=[(model.metrics_names[0],lossVal)])\n",
        "          lossHist.append(lossVal)\n",
        "        except Exception as e:\n",
        "          print(str(e))\n",
        "\n",
        "        #boostx : end  \n",
        "    if epoch % SAVE_EVERY_EPOCH == 0:\n",
        "        fileName = WEIGHTS_PATH+'model.epoch-{}.h5'.format(epoch)\n",
        "        print(\"\\nSaving Weights to:\", fileName)\n",
        "        model.save_weights(fileName, True)\n",
        "        \n",
        "        tStatus = {}\n",
        "        tStatus['epoch'] = epoch\n",
        "        tStatus['file-name'] = os.path.basename(fileName)\n",
        "        tStatus['loss'] = round(sum(lossHist)/len(lossHist),4)\n",
        "\n",
        "        with open(WEIGHTS_PATH+\"train-status.csv\",\"a\") as trainStatusFile:\n",
        "            tStatusLogger = csv.DictWriter(trainStatusFile, fieldnames=field_names)\n",
        "            tStatusLogger.writerow(tStatus)\n",
        "            \n",
        "\n",
        "#model.fit_generator(generator=train_generator,steps_per_epoch=128)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDcdv211FG_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from numpy import newaxis\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "def anomalyScore(y_true, y_pred):\n",
        "    mask = getMaskArray(IMAGE_SIZE, 24)\n",
        "    invMask = 1-mask\n",
        "    y_masked_true = np.multiply(y_true, mask)\n",
        "    y_masked_pred = np.multiply(y_pred, mask)\n",
        "    diff = ((y_masked_true-y_masked_pred)**2).mean()\n",
        "    return diff\n",
        "\n",
        "def testModel(model, dir):\n",
        "    #boostx : train_directory not defined error, define here, use the training directory fist. \n",
        "    train_directory=TEST_PATH\n",
        "    #boostx : end \n",
        "    folder=train_directory+dir\n",
        "\n",
        "    files = []\n",
        "    scores = [] \n",
        "    mask = getMaskArray(IMAGE_SIZE, MASK_SIZE)\n",
        "    invMask = 1-mask\n",
        "    mulMask = invMask[:, :, np.newaxis]\n",
        "\n",
        "    for filename in tqdm(os.listdir(folder)):\n",
        "        img = cv2.imread(os.path.join(folder,filename),0)\n",
        "        #boostx : to avoid broken image error\n",
        "        #print(str(filename))\n",
        "        try:\n",
        "          img = cv2.resize(img,(128,128))\n",
        "          imgScaled = (img.astype(np.float32) - 127.5) / 127.5\n",
        "          imgScaled = np.array([imgScaled])\n",
        "          imgScaled = imgScaled[:,:,:,newaxis]\n",
        "          imgMasked = np.multiply(imgScaled,mulMask)\n",
        "          imgPredict = model.predict(imgMasked)\n",
        "          score = anomalyScore(imgMasked, imgPredict)\n",
        "          files.append(filename)\n",
        "          scores.append(score)\n",
        "        except Exception as e:\n",
        "          print(str(e))\n",
        "        #boost x end\n",
        "        #plt.imshow(img, cmap='gray')\n",
        "        #plt.imshow(im3[0,:,:,0], cmap='gray')\n",
        "        #plt.imshow(imgPredict[0,:,:,0], cmap='gray')\n",
        "        #plt.show()\n",
        "    return pd.DataFrame({'files':files, 'scores':scores})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJG8omFZPL32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#boostx : uncommented technoDf\n",
        "technoDf = testModel(model,\"techno/4\")\n",
        "#technoDf[\"class\"] = \"techno\"\n",
        "#print(\"Technno:\\n\",technoDf)\n",
        "\n",
        "naturalDf = testModel(model,\"natural\")\n",
        "\n",
        "#naturalDf[\"class\"] = \"natural\"\n",
        "#print(\"Natural:\\n\",naturalDf)\n",
        "\n",
        "#resultsDf = technoDf.append(naturalDf,ignore_index =True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6owcexbPIosV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXa47oDad2S_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "naturalDf.hist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dfOV5BRd3di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "naturalDf.sort_values(\"scores\").tail(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaD3TE_mheHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfFileIndex = naturalDf.set_index(\"files\")\n",
        "print(dfFileIndex.head())\n",
        "for dir in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"):\n",
        "    print(\"Scores for:\", dir+\":\")\n",
        "    for file in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"+dir):\n",
        "        print(file,\":\",dfFileIndex.loc[file].scores)\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEaQYFZKiUui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#boostx : ignored this line   dfFileIndex.loc[\"img_x_11520_y_16512.jpg\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNavJGakdcwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "found = []\n",
        "notFound = []\n",
        "\n",
        "#boostx : define a cutOffScore\n",
        "cutOffScore = 0.001\n",
        "\n",
        "for img in technoDf[technoDf.scores>cutOffScore].files:\n",
        "    for dir in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"):\n",
        "        if dir not in notFound:\n",
        "            notFound.append(dir)\n",
        "\n",
        "for img in technoDf[technoDf.scores>cutOffScore].files:\n",
        "    for dir in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"):\n",
        "        if img in os.listdir(TMP_PATH+\"content/gdrive/My Drive/aiml/OmdenaMars/data/techno/\"+dir):\n",
        "            if dir not in found:\n",
        "                found.append(dir)\n",
        "                if dir in notFound:\n",
        "                    notFound.remove(dir)\n",
        "\n",
        "print(\"Anomaly detected:\",found)\n",
        "print(\"Anomaly not-detected:\",notFound)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e_XCWEQdqFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}